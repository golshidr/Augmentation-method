{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golshidr/Augmentation-method/blob/main/unbalanced_data_paper2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0XcMmkDhWS_"
      },
      "source": [
        "# **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ckLGsnUYhQlB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import numpy as np\n",
        "from imblearn.datasets import fetch_datasets\n",
        "from pandas.io.parsers.readers import read_csv\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,precision_score,recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn import preprocessing\n",
        "from scipy.io import arff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh7kSrXWx8Yi",
        "outputId": "cfdcb837-d24f-433f-c715-e21154e6684e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting arff\n",
            "  Downloading arff-0.9.tar.gz (4.7 kB)\n",
            "Building wheels for collected packages: arff\n",
            "  Building wheel for arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arff: filename=arff-0.9-py3-none-any.whl size=4969 sha256=dccf191738ba6ada218504ac23defd11ad73989d798a0ef6ece4a7fad9d95d23\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/5b/6a/fdaf9e4b202a61789ddbc0341e3a0df5406d8e36edf08feaec\n",
            "Successfully built arff\n",
            "Installing collected packages: arff\n",
            "Successfully installed arff-0.9\n"
          ]
        }
      ],
      "source": [
        "pip install arff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FyklrYyiyfv",
        "outputId": "56621191-5f0e-436b-b49b-9a6088175c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD9e-y9Hhc62"
      },
      "source": [
        "# **import dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UVCg9pIghZ4X"
      },
      "outputs": [],
      "source": [
        "#df=  pd.read_csv('/content/drive/MyDrive/crx (3).data', sep=\",\")\n",
        "df=pd.read_csv('/content/drive/MyDrive/american_companies.csv')\n",
        "df=df.loc[:, df.columns!='status_label']\n",
        "df=df.loc[:,df.columns != 'Unnamed: 0']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7hm4PILwYA0"
      },
      "source": [
        "# **General functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jvTUfb8qxxuE"
      },
      "outputs": [],
      "source": [
        "  model_pipeline=[]\n",
        "  model_pipeline.append(BernoulliNB())\n",
        "  model_pipeline.append(SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=150))\n",
        "  model_pipeline.append(KNeighborsClassifier(n_neighbors=5))\n",
        "  model_pipeline.append(MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(15,30), random_state=1,max_iter=1500))\n",
        "  model_pipeline.append(RandomForestClassifier(n_estimators=500,min_samples_split=5,min_samples_leaf=16,criterion='entropy'))\n",
        "  model_pipeline.append(SVC(kernel='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "e2Vq-jzJkgEy"
      },
      "outputs": [],
      "source": [
        "class evaluation:\n",
        "  def __init__(self):\n",
        "\n",
        "    self.result_df = pd.DataFrame()\n",
        "    self.result_df1 = {}\n",
        "\n",
        "  def metrics_computation(self, real_values,pred_values):\n",
        "\n",
        "    CM = confusion_matrix(real_values,pred_values)\n",
        "    TN = CM[0][0] \n",
        "    FN = CM[1][0] \n",
        "    TP = CM[1][1]\n",
        "    FP = CM[0][1]\n",
        "    precision_min = round( TP / (TP+FP),4 )\n",
        "    recall_min = round( TP / (TP+FN),4 )\n",
        "    f1_min = 2/((1/precision_min)+(1/recall_min))\n",
        "    return (precision_min,recall_min,f1_min)\n",
        "  \n",
        "  def fit(self, X, Y, name:str):\n",
        "    x = X\n",
        "    y = Y\n",
        "    acc_list_final=[]\n",
        "    pr_minority_list_final=[]\n",
        "    recall_minority_list_final=[]\n",
        "    f1_minority_list_final=[]\n",
        "\n",
        "    f1_list_final_micro=[]\n",
        "    f1_list_final_macro=[]\n",
        "    f1_list_final_weighted=[]\n",
        "\n",
        "    pr_list_final_micro=[]\n",
        "    pr_list_final_macro=[]\n",
        "    pr_list_final_weighted=[]\n",
        "\n",
        "    recall_list_final_micro=[]\n",
        "    recall_list_final_macro=[]\n",
        "    recall_list_final_weighted=[]\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=0)\n",
        "    for model in model_pipeline:\n",
        "      f1_minority_list=[]\n",
        "      f1_list_micro=[]\n",
        "      f1_list_macro=[]\n",
        "      f1_list_weighted=[]\n",
        "      acc_list=[]\n",
        "      pr_list_micro=[]\n",
        "      pr_list_macro=[]\n",
        "      pr_list_weighted=[]\n",
        "      recall_list_micro=[]\n",
        "      recall_list_macro=[]\n",
        "      recall_list_weighted=[]\n",
        "      pr_minority_list=[]\n",
        "      recall_minority_list=[]\n",
        "      model.fit(X_train,Y_train.ravel())\n",
        "      Y_pred_test=model.predict(X_test)\n",
        "      acc=accuracy_score(Y_test,Y_pred_test)\n",
        "      acc_list.append(round((acc),4))\n",
        "      acc_list_final.append(acc_list)\n",
        "\n",
        "      pr_minority, recall_minority,f1_minority = self.metrics_computation(Y_test,Y_pred_test)\n",
        "\n",
        "      pr_minority_list.append(round((pr_minority),4))\n",
        "      pr_minority_list_final.append(pr_minority_list)\n",
        "\n",
        "      recall_minority_list.append(round((recall_minority),4))\n",
        "      recall_minority_list_final.append(recall_minority_list)\n",
        "\n",
        "      f1_minority_list.append(round((f1_minority),4))\n",
        "      f1_minority_list_final.append(f1_minority_list)\n",
        "\n",
        "      f1=f1_score(Y_test,Y_pred_test,average='weighted')\n",
        "      f1_list_weighted.append(round((f1),4))\n",
        "      f1_list_final_weighted.append(f1_list_weighted)\n",
        "\n",
        "      f1=f1_score(Y_test,Y_pred_test,average='micro')\n",
        "      f1_list_micro.append(round((f1),4))\n",
        "      f1_list_final_micro.append(f1_list_micro)\n",
        "\n",
        "      f1=f1_score(Y_test,Y_pred_test,average='macro')\n",
        "      f1_list_macro.append(round((f1),4))\n",
        "      f1_list_final_macro.append(f1_list_macro)\n",
        "\n",
        "      pr=precision_score(Y_test,Y_pred_test,average='weighted')\n",
        "      pr_list_weighted.append(round((pr),4))\n",
        "      pr_list_final_weighted.append(pr_list_weighted)\n",
        "\n",
        "      pr=precision_score(Y_test,Y_pred_test,average='micro')\n",
        "      pr_list_micro.append(round((pr),4))\n",
        "      pr_list_final_micro.append(pr_list_micro)\n",
        "\n",
        "      pr=precision_score(Y_test,Y_pred_test,average='macro')\n",
        "      pr_list_macro.append(round((pr),4))\n",
        "      pr_list_final_macro.append(pr_list_macro)\n",
        "\n",
        "      recall=recall_score(Y_test,Y_pred_test,average='weighted')\n",
        "      recall_list_weighted.append(round((recall),4))\n",
        "      recall_list_final_weighted.append(recall_list_weighted)\n",
        "\n",
        "      recall=recall_score(Y_test,Y_pred_test,average='micro')\n",
        "      recall_list_micro.append(round((recall),4))\n",
        "      recall_list_final_micro.append(recall_list_micro)\n",
        "\n",
        "      recall=recall_score(Y_test,Y_pred_test,average='macro')\n",
        "      recall_list_macro.append(round((recall),4))\n",
        "      recall_list_final_macro.append(recall_list_macro)\n",
        "    \n",
        "    self.result_df1.update({'Model  ' + name :model_list, 'Accuracy'+ name:acc_list_final,'f1_score_micro'+ name:f1_list_final_micro,\n",
        "                                'f1_score_macro'+ name:f1_list_final_macro,'f1_score_weighted'+ name:f1_list_final_weighted,\n",
        "                                'precision_micro'+ name:pr_list_final_micro,'precision_macro'+ name:pr_list_final_macro,\n",
        "                                'precision_weighted'+ name:pr_list_final_weighted,'recall_micro'+ name:recall_list_final_micro,\n",
        "                                'recall_macro'+ name+ name:recall_list_final_macro,'recall_weighted'+ name:recall_list_final_weighted,\n",
        "                                'precision_minority class'+ name+ name:pr_minority_list_final,'F1_score_minority class'+ name:f1_minority_list_final,\n",
        "                                'recall_minority class'+ name:recall_minority_list_final})\n",
        "    return self.result_df1\n",
        "    \n",
        "\n",
        "\n",
        "  def show(self):\n",
        "    return self.result_df1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev = evaluation()"
      ],
      "metadata": {
        "id": "kDMTY4qoBEHR"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Augmentation functions**"
      ],
      "metadata": {
        "id": "HGwKq_LE_C7t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "YtV9T2EYhGRW"
      },
      "outputs": [],
      "source": [
        "def cluster_centroids_not_minority(df):\n",
        "  from imblearn.under_sampling import ClusterCentroids \n",
        "  sm = ClusterCentroids(random_state=42,sampling_strategy='not minority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='Cluster Centroids Not Minority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def cluster_centroids_not_majority(df):\n",
        "  from imblearn.under_sampling import ClusterCentroids \n",
        "  sm = ClusterCentroids(random_state=42,sampling_strategy='not majority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='Cluster Centroids Not Majority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def cluster_centroids_majority(df):\n",
        "  from imblearn.under_sampling import ClusterCentroids \n",
        "  sm = ClusterCentroids(random_state=42,sampling_strategy='majority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='Cluster Centroids Majority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def cluster_centroids_auto(df):\n",
        "  from imblearn.under_sampling import ClusterCentroids \n",
        "  sm = ClusterCentroids(random_state=42,sampling_strategy='auto')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='Cluster Centroids Auto'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def cluster_centroids_all(df):\n",
        "  from imblearn.under_sampling import ClusterCentroids \n",
        "  sm = ClusterCentroids(random_state=42,sampling_strategy='all')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='Cluster Centroids All'\n",
        "  ev.fit(X,Y, H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "kq-rydXAMtVA"
      },
      "outputs": [],
      "source": [
        "def SMOTE_minority(df):\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  sm = SMOTE(random_state=42,sampling_strategy='minority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='SMOTE_minority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def SMOTE_not_majority(df):\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  sm = SMOTE(random_state=42,sampling_strategy='not majority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='SMOTE Not Majority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def SMOTE_all(df):\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  sm = SMOTE(random_state=42,sampling_strategy='all')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='SMOTE All'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def SMOTE_not_minority(df):\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  sm = SMOTE(random_state=42,sampling_strategy='not minority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='SMOTE Not Minority'\n",
        "  ev.fit(X,Y, H)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "XhmZjAe9HbmA"
      },
      "outputs": [],
      "source": [
        "def ADASYN_not_minority(df):\n",
        "  from imblearn.over_sampling import ADASYN \n",
        "  sm = ADASYN(random_state=42,sampling_strategy='not minority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='ADASYN Not Minority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def ADASYN_minority(df):\n",
        "  from imblearn.over_sampling import ADASYN \n",
        "  sm = ADASYN(random_state=42,sampling_strategy='minority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='ADASYN Minority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def ADASYN_not_majority(df):\n",
        "  from imblearn.over_sampling import ADASYN \n",
        "  sm = ADASYN(random_state=42,sampling_strategy='not majority')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='ADASYN Not Majority'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "def ADASYN_all(df):\n",
        "  from imblearn.over_sampling import ADASYN \n",
        "  sm = ADASYN(random_state=42,sampling_strategy='all')\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  X, Y = sm.fit_resample(X, Y.ravel())\n",
        "  X=pd.DataFrame(X)\n",
        "  Y=pd.DataFrame(Y)\n",
        "  df_sm = pd.concat([X, Y], axis=1)\n",
        "  X=df_sm.loc[:, df_sm.columns != 'Class']\n",
        "  Y=df_sm.iloc[:,-1].values\n",
        "  H='ADASYN All'\n",
        "  ev.fit(X,Y, H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "CMveXFcetAfU"
      },
      "outputs": [],
      "source": [
        "def without_augmentation(df):\n",
        "  X=df.loc[:, df.columns != 'Class']\n",
        "  Y=df.iloc[:,-1].values\n",
        "  print('shape X',X.shape)\n",
        "  print('shape Y',Y.shape)\n",
        "  H='Without augmentation'\n",
        "  ev.fit(X,Y, H)\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "-X0ZGPb32Nzi"
      },
      "outputs": [],
      "source": [
        "def min_max(dfs):\n",
        "    for i in range(1):\n",
        "        index = 0\n",
        "        labels= df['Class'].unique()\n",
        "        df.rename(columns={ df.columns[-1]:'Class'}, inplace = True)\n",
        "        n_col=len(df.columns) - 1\n",
        "        dataframe_class = {}\n",
        "        lens={}\n",
        "  \n",
        "        for L in labels:\n",
        "          dataframe_for_label_L = df[df['Class'] == L]\n",
        "          dataframe_class[L] = pd.DataFrame(dataframe_for_label_L)\n",
        "          lens[L] = len(dataframe_for_label_L)\n",
        "\n",
        "        max_class = max(lens.values())\n",
        "        min_class = min(lens.values())\n",
        "        n= max_class - min_class\n",
        "        \n",
        "        q=min(lens.keys())\n",
        "        df_1=dataframe_class[q]\n",
        "\n",
        "        z=max(lens.keys())\n",
        "        df_0=dataframe_class[z]\n",
        "\n",
        "        new_data=pd.DataFrame()\n",
        "        dfs_0=pd.DataFrame()\n",
        "\n",
        "        j=0\n",
        "        while(index<=n_col):\n",
        "            colname = df_1.columns[index]\n",
        "            col = getattr(df_1, colname)\n",
        "            max_value = col.max()\n",
        "            min_value = col.min()\n",
        "            random_float_list = []\n",
        "            j=0\n",
        "            for i in range(0, n):\n",
        "              random_float_list.append(round(random.uniform(min_value, max_value), 5))\n",
        "              \n",
        "            np.reshape(random_float_list,(n,1))\n",
        "            new_data[df_1.columns[index]]=random_float_list\n",
        "            \n",
        "            index+=1\n",
        "            \n",
        "    new_data[new_data.columns[-1]] == 1\n",
        "    df_baseline=df.append(new_data)\n",
        "    \n",
        "    X=df_baseline.loc[:, df_baseline.columns != 'Class']\n",
        "    Y=df_baseline.iloc[:,-1].values\n",
        "\n",
        "    H='Baseline Min_max'\n",
        "    ev.fit(X,Y, H)\n",
        "\n",
        "def mean_std(dfs):\n",
        "    for i in range(1):\n",
        "        index = 0\n",
        "        labels= df['Class'].unique()\n",
        "        df.rename(columns={ df.columns[-1]:'Class'}, inplace = True)\n",
        "        n_col=len(df.columns) - 1\n",
        "        dataframe_class = {}\n",
        "        lens={}\n",
        "  \n",
        "        for L in labels:\n",
        "          dataframe_for_label_L = df[df['Class'] == L]\n",
        "          dataframe_class[L] = pd.DataFrame(dataframe_for_label_L)\n",
        "          lens[L] = len(dataframe_for_label_L)\n",
        "\n",
        "        max_class = max(lens.values())\n",
        "        min_class = min(lens.values())\n",
        "        n= max_class - min_class\n",
        "        \n",
        "        q=min(lens.keys())\n",
        "        df_1=dataframe_class[q]\n",
        "\n",
        "        z=max(lens.keys())\n",
        "        df_0=dataframe_class[z]\n",
        "\n",
        "        new_data=pd.DataFrame()\n",
        "        dfs_0=pd.DataFrame()\n",
        "\n",
        "        j=0\n",
        "        while(index<=n_col):\n",
        "            colname = df_1.columns[index]\n",
        "            col = getattr(df_1, colname)\n",
        "            mean_value = col.mean()\n",
        "            max_value = col.max()\n",
        "            std_value=col.std()\n",
        "            min_value = col.min()\n",
        "            mean_float_list = []\n",
        "            j=0\n",
        "            # Generating data in each column\n",
        "            for i in range(0, n):\n",
        "              z= mean_value - std_value\n",
        "              mean_float_list.append(z)\n",
        "              \n",
        "\n",
        "            random.shuffle(mean_float_list)  \n",
        "            np.reshape(mean_float_list,(n,1))\n",
        "            new_data[df_1.columns[index]]=mean_float_list\n",
        "            \n",
        "            index+=1\n",
        "            \n",
        "            \n",
        "    new_data[new_data.columns[-1]] == 1\n",
        "    df_baseline=df.append(new_data)\n",
        "    \n",
        "    X=df_baseline.loc[:, df_baseline.columns != 'Class']\n",
        "    Y=df_baseline.iloc[:,-1].values\n",
        "\n",
        "    H='Baseline Mean_std'\n",
        "    ev.fit(X,Y, H)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "cBCV6WFwzkP7"
      },
      "outputs": [],
      "source": [
        "from numpy.core.fromnumeric import shape\n",
        "def min_max_type2(dfs):\n",
        "  index = 0\n",
        "  labels= df['Class'].unique()\n",
        "  df.rename(columns={ df.columns[-1]:'Class'}, inplace = True)\n",
        "  n_col=len(df.columns) - 1\n",
        "  dataframe_class = {}\n",
        "  lens={}\n",
        "  n=2000\n",
        "\n",
        "  df_total=pd.DataFrame()      \n",
        "\n",
        "  for L in labels:\n",
        "    dataframe_for_label_L = df[df['Class'] == L]\n",
        "    dataframe_class[L] = pd.DataFrame(dataframe_for_label_L)\n",
        "    lens[L] = len(dataframe_for_label_L)\n",
        "    new_data=pd.DataFrame()\n",
        "    index=0\n",
        "    while(index<=n_col):\n",
        "      colname = dataframe_class[L].columns[index]\n",
        "      col = getattr(dataframe_class[L], colname)\n",
        "      max_value = col.max()\n",
        "      min_value = col.min()\n",
        "      random_float_list = []\n",
        "      \n",
        "\n",
        "      for i in range(0, n):\n",
        "        random_float_list.append(round(random.uniform(min_value, max_value), 5))\n",
        "\n",
        "      np.reshape(random_float_list,(n,1))\n",
        "      new_data[dataframe_class[L].columns[index]]=random_float_list       \n",
        "      index+=1\n",
        "\n",
        "      df_total=df_total.append(new_data)   \n",
        "    df_baseline=df.append(df_total)\n",
        "\n",
        "  class_0=df_baseline[df_baseline['Class']==0]\n",
        "  class_1=df_baseline[df_baseline['Class']==1]\n",
        "  print('class 0 type2',shape(class_0))\n",
        "  print('class 1 type2',shape(class_1))\n",
        "  df_baseline = df_baseline.fillna(0)\n",
        "\n",
        "  X=df_baseline.loc[:, df_baseline.columns != 'Class']\n",
        "  Y=df_baseline.iloc[:,-1].values\n",
        "\n",
        "  H='Baseline Min_max'\n",
        "  ev.fit(X,Y, H)\n",
        "\n",
        "\n",
        "def mean_std_type2(dfs):\n",
        "  index = 0\n",
        "  labels= df['Class'].unique()\n",
        "  df.rename(columns={ df.columns[-1]:'Class'}, inplace = True)\n",
        "  n_col=len(df.columns) - 1\n",
        "  dataframe_class = {}\n",
        "  lens={}\n",
        "  n=2000\n",
        "  print('Baseline mean-std type 2')\n",
        "  print('orginal df',shape(df))\n",
        "  df_total=pd.DataFrame()      \n",
        "\n",
        "  for L in labels:\n",
        "    dataframe_for_label_L = df[df['Class'] == L]\n",
        "    dataframe_class[L] = pd.DataFrame(dataframe_for_label_L)\n",
        "    lens[L] = len(dataframe_for_label_L)\n",
        "    new_data=pd.DataFrame()\n",
        "    index=0\n",
        "    while(index<=n_col):\n",
        "      colname = dataframe_class[L].columns[index]\n",
        "      col = getattr(dataframe_class[L], colname)\n",
        "      mean_value = col.mean()\n",
        "      max_value = col.max()\n",
        "      std_value=col.std()\n",
        "      min_value = col.min()\n",
        "      mean_float_list = []\n",
        "      for i in range(0, n):\n",
        "        z= mean_value - std_value\n",
        "        mean_float_list.append(z)\n",
        "\n",
        "      np.reshape(mean_float_list,(n,1))\n",
        "      new_data[dataframe_class[L].columns[index]]=mean_float_list      \n",
        "      index+=1\n",
        "\n",
        "      df_total=df_total.append(new_data)   \n",
        "    df_baseline=df.append(df_total)\n",
        "\n",
        "  class_0=df_baseline[df_baseline['Class']==0]\n",
        "  class_1=df_baseline[df_baseline['Class']==1]\n",
        "  print('class 0 type2',shape(class_0))\n",
        "  print('class 1 type2',shape(class_1))\n",
        "  df_baseline = df_baseline.fillna(0)\n",
        "\n",
        "  X=df_baseline.loc[:, df_baseline.columns != 'Class']\n",
        "  Y=df_baseline.iloc[:,-1].values\n",
        "\n",
        "  H='Baseline Mean_Std'\n",
        "  ev.fit(X,Y, H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igrRYeH9mEEu"
      },
      "source": [
        "# **Defining class of augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnVzg951jbve",
        "outputId": "25f6e41e-7430-4153-f249-c521b2b5d282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Credit Card Approval Dataset\n",
            "Number of Classes: 2\n",
            "   Class  Percentage\n",
            "0   7700    0.932091\n",
            "1    561    0.067909\n",
            "shape X (8261, 18)\n",
            "shape Y (8261,)\n",
            "DataFrame is UNBALANCE and need Augmentation!\n",
            "it is NOT possible to perform a down-sampling of the samples belonging to the majority class(type1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Model  Without augmentation': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracyWithout augmentation': [[0.9295],\n",
              "  [0.9014],\n",
              "  [0.9337],\n",
              "  [0.9201],\n",
              "  [0.9304],\n",
              "  [0.918]],\n",
              " 'f1_score_microWithout augmentation': [[0.9295],\n",
              "  [0.9014],\n",
              "  [0.9337],\n",
              "  [0.9201],\n",
              "  [0.9304],\n",
              "  [0.918]],\n",
              " 'f1_score_macroWithout augmentation': [[0.4817],\n",
              "  [0.6161],\n",
              "  [0.5632],\n",
              "  [0.5861],\n",
              "  [0.482],\n",
              "  [0.4823]],\n",
              " 'f1_score_weightedWithout augmentation': [[0.8964],\n",
              "  [0.901],\n",
              "  [0.9095],\n",
              "  [0.9062],\n",
              "  [0.8969],\n",
              "  [0.8911]],\n",
              " 'precision_microWithout augmentation': [[0.9295],\n",
              "  [0.9014],\n",
              "  [0.9337],\n",
              "  [0.9201],\n",
              "  [0.9304],\n",
              "  [0.918]],\n",
              " 'precision_macroWithout augmentation': [[0.4652],\n",
              "  [0.617],\n",
              "  [0.8068],\n",
              "  [0.6395],\n",
              "  [0.4652],\n",
              "  [0.4765]],\n",
              " 'precision_weightedWithout augmentation': [[0.8656],\n",
              "  [0.9006],\n",
              "  [0.9182],\n",
              "  [0.8976],\n",
              "  [0.8657],\n",
              "  [0.8667]],\n",
              " 'recall_microWithout augmentation': [[0.9295],\n",
              "  [0.9014],\n",
              "  [0.9337],\n",
              "  [0.9201],\n",
              "  [0.9304],\n",
              "  [0.918]],\n",
              " 'recall_macroWithout augmentationWithout augmentation': [[0.4995],\n",
              "  [0.6151],\n",
              "  [0.544],\n",
              "  [0.5669],\n",
              "  [0.5],\n",
              "  [0.4953]],\n",
              " 'recall_weightedWithout augmentation': [[0.9295],\n",
              "  [0.9014],\n",
              "  [0.9337],\n",
              "  [0.9201],\n",
              "  [0.9304],\n",
              "  [0.918]],\n",
              " 'precision_minority classWithout augmentationWithout augmentation': [[0.0],\n",
              "  [0.2876],\n",
              "  [0.6774],\n",
              "  [0.3396],\n",
              "  [nan],\n",
              "  [0.0233]],\n",
              " 'F1_score_minority classWithout augmentation': [[0.0],\n",
              "  [0.2851],\n",
              "  [0.1609],\n",
              "  [0.2143],\n",
              "  [nan],\n",
              "  [0.0073]],\n",
              " 'recall_minority classWithout augmentation': [[0.0],\n",
              "  [0.2826],\n",
              "  [0.0913],\n",
              "  [0.1565],\n",
              "  [0.0],\n",
              "  [0.0043]],\n",
              " 'Model  SMOTE Not Minority': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracySMOTE Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [0.9994],\n",
              "  [0.918]],\n",
              " 'f1_score_microSMOTE Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [0.9994],\n",
              "  [0.918]],\n",
              " 'f1_score_macroSMOTE Not Minority': [[1.0],\n",
              "  [0.6217],\n",
              "  [0.5632],\n",
              "  [0.5795],\n",
              "  [0.9977],\n",
              "  [0.4823]],\n",
              " 'f1_score_weightedSMOTE Not Minority': [[1.0],\n",
              "  [0.9009],\n",
              "  [0.9095],\n",
              "  [0.9037],\n",
              "  [0.9994],\n",
              "  [0.8911]],\n",
              " 'precision_microSMOTE Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [0.9994],\n",
              "  [0.918]],\n",
              " 'precision_macroSMOTE Not Minority': [[1.0],\n",
              "  [0.6191],\n",
              "  [0.8068],\n",
              "  [0.6216],\n",
              "  [0.9997],\n",
              "  [0.4765]],\n",
              " 'precision_weightedSMOTE Not Minority': [[1.0],\n",
              "  [0.902],\n",
              "  [0.9182],\n",
              "  [0.8947],\n",
              "  [0.9994],\n",
              "  [0.8667]],\n",
              " 'recall_microSMOTE Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [0.9994],\n",
              "  [0.918]],\n",
              " 'recall_macroSMOTE Not MinoritySMOTE Not Minority': [[1.0],\n",
              "  [0.6244],\n",
              "  [0.544],\n",
              "  [0.5631],\n",
              "  [0.9957],\n",
              "  [0.4953]],\n",
              " 'recall_weightedSMOTE Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [0.9994],\n",
              "  [0.918]],\n",
              " 'precision_minority classSMOTE Not MinoritySMOTE Not Minority': [[1.0],\n",
              "  [0.2905],\n",
              "  [0.6774],\n",
              "  [0.3043],\n",
              "  [1.0],\n",
              "  [0.0233]],\n",
              " 'F1_score_minority classSMOTE Not Minority': [[1.0],\n",
              "  [0.2972],\n",
              "  [0.1609],\n",
              "  [0.2029],\n",
              "  [0.9956],\n",
              "  [0.0073]],\n",
              " 'recall_minority classSMOTE Not Minority': [[1.0],\n",
              "  [0.3043],\n",
              "  [0.0913],\n",
              "  [0.1522],\n",
              "  [0.9913],\n",
              "  [0.0043]],\n",
              " 'Model  SMOTE All': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracySMOTE All': [[1.0], [0.6718], [0.8695], [0.8032], [1.0], [0.5073]],\n",
              " 'f1_score_microSMOTE All': [[1.0],\n",
              "  [0.6718],\n",
              "  [0.8695],\n",
              "  [0.8032],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'f1_score_macroSMOTE All': [[1.0],\n",
              "  [0.6675],\n",
              "  [0.8677],\n",
              "  [0.8012],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'f1_score_weightedSMOTE All': [[1.0],\n",
              "  [0.6674],\n",
              "  [0.8678],\n",
              "  [0.8013],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'precision_microSMOTE All': [[1.0],\n",
              "  [0.6718],\n",
              "  [0.8695],\n",
              "  [0.8032],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'precision_macroSMOTE All': [[1.0],\n",
              "  [0.6818],\n",
              "  [0.8894],\n",
              "  [0.8153],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'precision_weightedSMOTE All': [[1.0],\n",
              "  [0.682],\n",
              "  [0.889],\n",
              "  [0.8151],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'recall_microSMOTE All': [[1.0],\n",
              "  [0.6718],\n",
              "  [0.8695],\n",
              "  [0.8032],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'recall_macroSMOTE AllSMOTE All': [[1.0],\n",
              "  [0.6722],\n",
              "  [0.869],\n",
              "  [0.8029],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'recall_weightedSMOTE All': [[1.0],\n",
              "  [0.6718],\n",
              "  [0.8695],\n",
              "  [0.8032],\n",
              "  [1.0],\n",
              "  [0.5073]],\n",
              " 'precision_minority classSMOTE AllSMOTE All': [[1.0],\n",
              "  [0.7255],\n",
              "  [0.8023],\n",
              "  [0.7546],\n",
              "  [1.0],\n",
              "  [0.5093]],\n",
              " 'F1_score_minority classSMOTE All': [[1.0],\n",
              "  [0.6299],\n",
              "  [0.8831],\n",
              "  [0.8213],\n",
              "  [1.0],\n",
              "  [0.5075]],\n",
              " 'recall_minority classSMOTE All': [[1.0],\n",
              "  [0.5566],\n",
              "  [0.9819],\n",
              "  [0.901],\n",
              "  [1.0],\n",
              "  [0.5058]],\n",
              " 'Model  ADASYN Not Minority': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracyADASYN Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [1.0],\n",
              "  [0.918]],\n",
              " 'f1_score_microADASYN Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [1.0],\n",
              "  [0.918]],\n",
              " 'f1_score_macroADASYN Not Minority': [[1.0],\n",
              "  [0.5962],\n",
              "  [0.5632],\n",
              "  [0.5795],\n",
              "  [1.0],\n",
              "  [0.4823]],\n",
              " 'f1_score_weightedADASYN Not Minority': [[1.0],\n",
              "  [0.8976],\n",
              "  [0.9095],\n",
              "  [0.9037],\n",
              "  [1.0],\n",
              "  [0.8911]],\n",
              " 'precision_microADASYN Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [1.0],\n",
              "  [0.918]],\n",
              " 'precision_macroADASYN Not Minority': [[1.0],\n",
              "  [0.6008],\n",
              "  [0.8068],\n",
              "  [0.6216],\n",
              "  [1.0],\n",
              "  [0.4765]],\n",
              " 'precision_weightedADASYN Not Minority': [[1.0],\n",
              "  [0.8955],\n",
              "  [0.9182],\n",
              "  [0.8947],\n",
              "  [1.0],\n",
              "  [0.8667]],\n",
              " 'recall_microADASYN Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [1.0],\n",
              "  [0.918]],\n",
              " 'recall_macroADASYN Not MinorityADASYN Not Minority': [[1.0],\n",
              "  [0.5922],\n",
              "  [0.544],\n",
              "  [0.5631],\n",
              "  [1.0],\n",
              "  [0.4953]],\n",
              " 'recall_weightedADASYN Not Minority': [[1.0],\n",
              "  [0.8998],\n",
              "  [0.9337],\n",
              "  [0.9168],\n",
              "  [1.0],\n",
              "  [0.918]],\n",
              " 'precision_minority classADASYN Not MinorityADASYN Not Minority': [[1.0],\n",
              "  [0.2584],\n",
              "  [0.6774],\n",
              "  [0.3043],\n",
              "  [1.0],\n",
              "  [0.0233]],\n",
              " 'F1_score_minority classADASYN Not Minority': [[1.0],\n",
              "  [0.246],\n",
              "  [0.1609],\n",
              "  [0.2029],\n",
              "  [1.0],\n",
              "  [0.0073]],\n",
              " 'recall_minority classADASYN Not Minority': [[1.0],\n",
              "  [0.2348],\n",
              "  [0.0913],\n",
              "  [0.1522],\n",
              "  [1.0],\n",
              "  [0.0043]],\n",
              " 'Model  ADASYN All': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracyADASYN All': [[1.0], [0.695], [0.8682], [0.7714], [1.0], [0.5245]],\n",
              " 'f1_score_microADASYN All': [[1.0],\n",
              "  [0.695],\n",
              "  [0.8682],\n",
              "  [0.7714],\n",
              "  [1.0],\n",
              "  [0.5245]],\n",
              " 'f1_score_macroADASYN All': [[1.0],\n",
              "  [0.6919],\n",
              "  [0.8664],\n",
              "  [0.7713],\n",
              "  [1.0],\n",
              "  [0.5244]],\n",
              " 'f1_score_weightedADASYN All': [[1.0],\n",
              "  [0.6918],\n",
              "  [0.8663],\n",
              "  [0.7713],\n",
              "  [1.0],\n",
              "  [0.5244]],\n",
              " 'precision_microADASYN All': [[1.0],\n",
              "  [0.695],\n",
              "  [0.8682],\n",
              "  [0.7714],\n",
              "  [1.0],\n",
              "  [0.5245]],\n",
              " 'precision_macroADASYN All': [[1.0],\n",
              "  [0.7041],\n",
              "  [0.8909],\n",
              "  [0.7719],\n",
              "  [1.0],\n",
              "  [0.5245]],\n",
              " 'precision_weightedADASYN All': [[1.0],\n",
              "  [0.7043],\n",
              "  [0.8913],\n",
              "  [0.7719],\n",
              "  [1.0],\n",
              "  [0.5245]],\n",
              " 'recall_microADASYN All': [[1.0],\n",
              "  [0.695],\n",
              "  [0.8682],\n",
              "  [0.7714],\n",
              "  [1.0],\n",
              "  [0.5245]],\n",
              " 'recall_macroADASYN AllADASYN All': [[1.0],\n",
              "  [0.6955],\n",
              "  [0.8687],\n",
              "  [0.7715],\n",
              "  [1.0],\n",
              "  [0.5245]],\n",
              " 'recall_weightedADASYN All': [[1.0],\n",
              "  [0.695],\n",
              "  [0.8682],\n",
              "  [0.7714],\n",
              "  [1.0],\n",
              "  [0.5245]],\n",
              " 'precision_minority classADASYN AllADASYN All': [[1.0],\n",
              "  [0.6599],\n",
              "  [0.7955],\n",
              "  [0.7591],\n",
              "  [1.0],\n",
              "  [0.5229]],\n",
              " 'F1_score_minority classADASYN All': [[1.0],\n",
              "  [0.7228],\n",
              "  [0.8819],\n",
              "  [0.7752],\n",
              "  [1.0],\n",
              "  [0.5167]],\n",
              " 'recall_minority classADASYN All': [[1.0],\n",
              "  [0.7989],\n",
              "  [0.9894],\n",
              "  [0.792],\n",
              "  [1.0],\n",
              "  [0.5106]],\n",
              " 'Model  Cluster Centroids Not Minority': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracyCluster Centroids Not Minority': [[0.9933],\n",
              "  [0.8396],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.9755],\n",
              "  [0.8797]],\n",
              " 'f1_score_microCluster Centroids Not Minority': [[0.9933],\n",
              "  [0.8396],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.9755],\n",
              "  [0.8797]],\n",
              " 'f1_score_macroCluster Centroids Not Minority': [[0.9933],\n",
              "  [0.8392],\n",
              "  [0.9197],\n",
              "  [0.8507],\n",
              "  [0.9754],\n",
              "  [0.8794]],\n",
              " 'f1_score_weightedCluster Centroids Not Minority': [[0.9933],\n",
              "  [0.8389],\n",
              "  [0.9198],\n",
              "  [0.8506],\n",
              "  [0.9755],\n",
              "  [0.8797]],\n",
              " 'precision_microCluster Centroids Not Minority': [[0.9933],\n",
              "  [0.8396],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.9755],\n",
              "  [0.8797]],\n",
              " 'precision_macroCluster Centroids Not Minority': [[0.9937],\n",
              "  [0.85],\n",
              "  [0.9196],\n",
              "  [0.8546],\n",
              "  [0.9758],\n",
              "  [0.8798]],\n",
              " 'precision_weightedCluster Centroids Not Minority': [[0.9934],\n",
              "  [0.8531],\n",
              "  [0.92],\n",
              "  [0.8566],\n",
              "  [0.9756],\n",
              "  [0.8798]],\n",
              " 'recall_microCluster Centroids Not Minority': [[0.9933],\n",
              "  [0.8396],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.9755],\n",
              "  [0.8797]],\n",
              " 'recall_macroCluster Centroids Not MinorityCluster Centroids Not Minority': [[0.993],\n",
              "  [0.8433],\n",
              "  [0.9201],\n",
              "  [0.8531],\n",
              "  [0.9752],\n",
              "  [0.8791]],\n",
              " 'recall_weightedCluster Centroids Not Minority': [[0.9933],\n",
              "  [0.8396],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.9755],\n",
              "  [0.8797]],\n",
              " 'precision_minority classCluster Centroids Not MinorityCluster Centroids Not Minority': [[0.9873],\n",
              "  [0.9219],\n",
              "  [0.9304],\n",
              "  [0.9034],\n",
              "  [0.9705],\n",
              "  [0.8782]],\n",
              " 'F1_score_minority classCluster Centroids Not Minority': [[0.9936],\n",
              "  [0.831],\n",
              "  [0.9224],\n",
              "  [0.8481],\n",
              "  [0.9767],\n",
              "  [0.8856]],\n",
              " 'recall_minority classCluster Centroids Not Minority': [[1.0],\n",
              "  [0.7564],\n",
              "  [0.9145],\n",
              "  [0.7991],\n",
              "  [0.9829],\n",
              "  [0.8932]],\n",
              " 'Model  Cluster Centroids Auto': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracyCluster Centroids Auto': [[0.9933],\n",
              "  [0.588],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.971],\n",
              "  [0.8797]],\n",
              " 'f1_score_microCluster Centroids Auto': [[0.9933],\n",
              "  [0.588],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.971],\n",
              "  [0.8797]],\n",
              " 'f1_score_macroCluster Centroids Auto': [[0.9933],\n",
              "  [0.5297],\n",
              "  [0.9197],\n",
              "  [0.8507],\n",
              "  [0.971],\n",
              "  [0.8794]],\n",
              " 'f1_score_weightedCluster Centroids Auto': [[0.9933],\n",
              "  [0.5227],\n",
              "  [0.9198],\n",
              "  [0.8506],\n",
              "  [0.971],\n",
              "  [0.8797]],\n",
              " 'precision_microCluster Centroids Auto': [[0.9933],\n",
              "  [0.588],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.971],\n",
              "  [0.8797]],\n",
              " 'precision_macroCluster Centroids Auto': [[0.9937],\n",
              "  [0.734],\n",
              "  [0.9196],\n",
              "  [0.8546],\n",
              "  [0.9713],\n",
              "  [0.8798]],\n",
              " 'precision_weightedCluster Centroids Auto': [[0.9934],\n",
              "  [0.7423],\n",
              "  [0.92],\n",
              "  [0.8566],\n",
              "  [0.9711],\n",
              "  [0.8798]],\n",
              " 'recall_microCluster Centroids Auto': [[0.9933],\n",
              "  [0.588],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.971],\n",
              "  [0.8797]],\n",
              " 'recall_macroCluster Centroids AutoCluster Centroids Auto': [[0.993],\n",
              "  [0.6039],\n",
              "  [0.9201],\n",
              "  [0.8531],\n",
              "  [0.9707],\n",
              "  [0.8791]],\n",
              " 'recall_weightedCluster Centroids Auto': [[0.9933],\n",
              "  [0.588],\n",
              "  [0.9198],\n",
              "  [0.8508],\n",
              "  [0.971],\n",
              "  [0.8797]],\n",
              " 'precision_minority classCluster Centroids AutoCluster Centroids Auto': [[0.9873],\n",
              "  [0.9298],\n",
              "  [0.9304],\n",
              "  [0.9034],\n",
              "  [0.9662],\n",
              "  [0.8782]],\n",
              " 'F1_score_minority classCluster Centroids Auto': [[0.9936],\n",
              "  [0.3643],\n",
              "  [0.9224],\n",
              "  [0.8481],\n",
              "  [0.9724],\n",
              "  [0.8856]],\n",
              " 'recall_minority classCluster Centroids Auto': [[1.0],\n",
              "  [0.2265],\n",
              "  [0.9145],\n",
              "  [0.7991],\n",
              "  [0.9786],\n",
              "  [0.8932]],\n",
              " 'Model  Cluster Centroids All': ['naive bayes',\n",
              "  'SGD',\n",
              "  'KNN',\n",
              "  'MLP',\n",
              "  'random forest',\n",
              "  'svm'],\n",
              " 'AccuracyCluster Centroids All': [[0.9933],\n",
              "  [0.637],\n",
              "  [0.9176],\n",
              "  [0.7929],\n",
              "  [0.9733],\n",
              "  [0.8797]],\n",
              " 'f1_score_microCluster Centroids All': [[0.9933],\n",
              "  [0.637],\n",
              "  [0.9176],\n",
              "  [0.7929],\n",
              "  [0.9733],\n",
              "  [0.8797]],\n",
              " 'f1_score_macroCluster Centroids All': [[0.9933],\n",
              "  [0.6015],\n",
              "  [0.9175],\n",
              "  [0.7903],\n",
              "  [0.9732],\n",
              "  [0.8794]],\n",
              " 'f1_score_weightedCluster Centroids All': [[0.9933],\n",
              "  [0.5964],\n",
              "  [0.9176],\n",
              "  [0.7893],\n",
              "  [0.9733],\n",
              "  [0.8797]],\n",
              " 'precision_microCluster Centroids All': [[0.9933],\n",
              "  [0.637],\n",
              "  [0.9176],\n",
              "  [0.7929],\n",
              "  [0.9733],\n",
              "  [0.8797]],\n",
              " 'precision_macroCluster Centroids All': [[0.9937],\n",
              "  [0.7545],\n",
              "  [0.9174],\n",
              "  [0.821],\n",
              "  [0.9734],\n",
              "  [0.8798]],\n",
              " 'precision_weightedCluster Centroids All': [[0.9934],\n",
              "  [0.7622],\n",
              "  [0.9176],\n",
              "  [0.8255],\n",
              "  [0.9733],\n",
              "  [0.8798]],\n",
              " 'recall_microCluster Centroids All': [[0.9933],\n",
              "  [0.637],\n",
              "  [0.9176],\n",
              "  [0.7929],\n",
              "  [0.9733],\n",
              "  [0.8797]],\n",
              " 'recall_macroCluster Centroids AllCluster Centroids All': [[0.993],\n",
              "  [0.6508],\n",
              "  [0.9175],\n",
              "  [0.799],\n",
              "  [0.973],\n",
              "  [0.8791]],\n",
              " 'recall_weightedCluster Centroids All': [[0.9933],\n",
              "  [0.637],\n",
              "  [0.9176],\n",
              "  [0.7929],\n",
              "  [0.9733],\n",
              "  [0.8797]],\n",
              " 'precision_minority classCluster Centroids AllCluster Centroids All': [[0.9873],\n",
              "  [0.9383],\n",
              "  [0.9227],\n",
              "  [0.9273],\n",
              "  [0.9703],\n",
              "  [0.8782]],\n",
              " 'F1_score_minority classCluster Centroids All': [[0.9936],\n",
              "  [0.4826],\n",
              "  [0.9207],\n",
              "  [0.7669],\n",
              "  [0.9744],\n",
              "  [0.8856]],\n",
              " 'recall_minority classCluster Centroids All': [[1.0],\n",
              "  [0.3248],\n",
              "  [0.9188],\n",
              "  [0.6538],\n",
              "  [0.9786],\n",
              "  [0.8932]]}"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "df.rename(columns={ df.columns[-1]:'Class'}, inplace = True)\n",
        "stat_df=df['Class'].value_counts()\n",
        "\n",
        "model_list=['naive bayes','SGD','KNN','MLP','random forest','svm']\n",
        "\n",
        "stat_df=pd.DataFrame(stat_df)\n",
        "stat_df['Percentage']=stat_df['Class']/stat_df['Class'].sum()\n",
        "print('Credit Card Approval Dataset')\n",
        "print('Number of Classes:',len(stat_df))\n",
        "print(stat_df.head(2))\n",
        "Y=(stat_df['Class'].max())*0.4\n",
        "without_augmentation(df)\n",
        "\n",
        "if stat_df['Class'].max() - Y > stat_df['Class'].min():\n",
        "  print('DataFrame is UNBALANCE and need Augmentation!')\n",
        "\n",
        "  if stat_df['Class'].min() > 5000:\n",
        "    print('it is possible to perform a down-sampling of the samples belonging to the majority class(type3)')\n",
        "\n",
        "    SMOTE_minority(df)\n",
        "    SMOTE_not_majority(df)\n",
        "    SMOTE_all(df)\n",
        "    ADASYN_minority(df)\n",
        "    ADASYN_not_majority(df)\n",
        "    ADASYN_all(df)\n",
        "    min_max(df)\n",
        "    mean_std(df)\n",
        "    cluster_centroids_not_majority(df)\n",
        "    cluster_centroids_majority(df)\n",
        "    cluster_centroids_auto(df) \n",
        "    cluster_centroids_all(df)  \n",
        "\n",
        "  else:\n",
        "    print('it is NOT possible to perform a down-sampling of the samples belonging to the majority class(type1)')\n",
        " \n",
        "    SMOTE_not_minority(df)\n",
        "    SMOTE_all(df)\n",
        "    ADASYN_not_minority(df)\n",
        "    ADASYN_all(df)\n",
        "    cluster_centroids_not_minority(df)\n",
        "    cluster_centroids_auto(df) \n",
        "    cluster_centroids_all(df)  \n",
        "  \n",
        "else:\n",
        "  print('DataFrame is BALANCE and Does Not need Augmentation!')\n",
        "  if stat_df['Class'].min() < 500:\n",
        "    print('samples are SMALL for machine learning models to work efficiently(type2)')\n",
        "\n",
        "    min_max_type2(df)\n",
        "    mean_std_type2(df)\n",
        "\n",
        "  else:\n",
        "    print('samples are ENOUGH for machine learning models to work efficiently(normal)')\n",
        "\n",
        "ev.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bdy_KZr7I8t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = ev.show()\n",
        "x = pd.DataFrame(x)\n",
        "x.to_csv('/content/drive/MyDrive/FinalResult.csv')"
      ],
      "metadata": {
        "id": "vXRV_v3oEd1Z"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gCzraSZljEF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k0XcMmkDhWS_",
        "HGwKq_LE_C7t"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}